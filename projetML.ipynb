{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fluid-israeli"
      },
      "source": [
        "# Machine Learning & Application \n",
        "## 2021-2022\n",
        "# Projet Rubik's cube\n",
        "\n",
        "Le projet porte sur un problème d'apprentissage supervisé. Le problème fait parti des données du [challenge des données](https://challengedata.ens.fr/challenges/20) ENS et s'intitule \"Solve 2x2x2 Rubik's cube\" et est présenté par la société LumenAI. Une [vidéo](https://www.college-de-france.fr/video/stephane-mallat/2019) décrivant le problème se trouve sur le site du collège de France. \n",
        "\n",
        "Les autres challenges sont aussi très intéressants, mais nécessitent plus de connaissances en machine learning (par exemple de l'apprentissage sur des séries temporelles, sur des images, des sons, du texte, etc...). D'où le choix de ce challenge dont les données sont très proches de problèmes sur lesquels on a travaillé.\n",
        "\n",
        "La résolution d'un rubik's cube peut être vu comme un problème d'intelligence artificielle (par exemple en utilisant des techniques de recherches avec des heuristiques). On peut même étudier le graphe du jeu du point de vue de la théorie des graphes et découvrir qu'en fait il existe toujours un chemin relativement court à une solution.\n",
        "\n",
        "ex dans la litérature:\n",
        " * \"The Diameter of the Rubik's Cube Group Is Twenty\", *T. Rokicki, H. Kociemba, M. Davidson, and J. Dethridge*, SIAM Review, 2014, Vol. 56, No. 4 : pp. 645-670.\n",
        " * \"Solving Rubik’s Cube Using Graph Theory\", Khemani C., Doshi J., Duseja J., Shah K., Udmale S., Sambhe V. (2019) in: Verma N., Ghosh A. (eds) Computational Intelligence: Theories, Applications and Future Directions - Volume I. Advances in Intelligent Systems and Computing, vol 798. Springer, Singapore. \n",
        " \n",
        "Ici, on a une base de données qui contient la description de rubik's cubes ainsi que le nombre minimal de coups pour les résoudre. On ne sait pas comment les problèmes ont été générés (est-ce que cela entraine un biais dans les problèmes, est-ce que plusieurs problèmes similaires sont présents dans la base? (Ici par similaire, on pourrait peut être avoir deux problèmes qui apparaissent dans la base, mais en permuttant certaines couleurs, on aurait peut-être exactement le même problème!)). Cependant, on vous demande de constuire un modèle pour nous aider à prédire le nombre de coups minimal pour un problème donné. Ensuite, vous pourriez utiliser ce modèle dans un algorithme de recherche étudié en cours d'IA.\n",
        "\n",
        "On peut le voir comme un problème de regression où il faut deviner le nombre minimal de coups pour résoudre le rubik's cube, ou comme un problème de classification où la classe d'un état du rubik's cube est le nombre minimal de coups pour le résoudre (donc on pourrait avoir au plus 19 classes). Toutes les méthodes que l'on a vu en cours peuvent s'appliquer.\n",
        "\n",
        "## Les données et le site du challenge\n",
        "\n",
        "Le projet s'effectue en binôme. Vous devez ouvrir un compte pour le binôme sur le site du challenge, choisissez de participer seul (le binôme sera un seul participant au challenge), puis inscrivez-vous au challenge du cours *M1 MIAGE Dauphine - PSL - 2021-2022*.\n",
        "\n",
        "Vous aurez accès à trois ensembles: \n",
        " * `x_train` qui contient la description de 1.837.079 différents rubik's cubes. Chaque rubik's cube est représenté par 25 attributs (lisez la description sur le site du challenge).\n",
        " * `y_train` qui contient le nombre minimal de coups pour résoudre chacun des 1.837.079 différents rubik's cubes. \n",
        " Ces données sont vos données d'entrainement.\n",
        " * enfin `x_test` qui contient la description de 1.837.080 nouveaux rubik's cubes. Vous ne connaissez pas le nombre minimal pour chacun de ces problèmes. \n",
        " \n",
        "Pour participer aux challenges, il vous faudra uploader sur le site votre prédiction sur les rubik's cubes du fichier `x_test` et le site du challenge vous donnera un score. Pour ce score, le site utilise l'erreur moyenne absolue: pour les n=1.837.080 exemples du fichier test, on fait la moyenne entre le vrai nombre minimal de coups $y_i$ et votre prédiction $z_i$: \n",
        "$$ \\frac{\\sum_{i=1}^n |y_i-z_i|}{n}$$\n",
        "\n",
        "Malheureusement (pour vous), le site ne vous donnera pas plus d'information que votre score, vous ne pourrez pas savoir quelles sont vos bonnes prédictions et quelles sont vos erreurs. Pire, le site vous permettra d'uploader une prédiction que deux fois par jour!"
      ],
      "id": "fluid-israeli"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfYQIj-X84GA",
        "outputId": "3691301d-0084-4850-f817-544d04acab69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "bfYQIj-X84GA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acknowledged-battery",
        "outputId": "8ec90191-3f56-4d2b-d89f-792200caaade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "taille des données:\n",
            "X: (1837079, 25)\n",
            "Y: (1837079, 2)\n",
            "test: (1837080, 25)\n",
            "Modèle RdmF\n",
            "Début de la phase d'entrainement\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "'''from sklearn import neighbors\n",
        "from sklearn.neighbors import KNeighborsClassifier'''\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/x_train.csv\") # remplacer par le chemin local\n",
        "X = df.to_numpy()\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/y_train.csv\")\n",
        "y = df.to_numpy()\n",
        "print(\"taille des données:\")\n",
        "print(\"X:\", X.shape)\n",
        "print(\"Y:\", y.shape)\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/x_test.csv\")\n",
        "Xtest = df.to_numpy()\n",
        "print(\"test:\", Xtest.shape)\n",
        "\n",
        "# Début du modèle \n",
        "\n",
        "#Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.20)\n",
        "\n",
        "Xtrain = X\n",
        "ytrain = y\n",
        "\n",
        "\n",
        "def take_first_col(data):\n",
        "    data_col = []\n",
        "    for i in range(len(data)):\n",
        "        data_col.append(data[i][0])\n",
        "    return data_col\n",
        "\n",
        "Xtest_indice = take_first_col(Xtest)\n",
        "\n",
        "\n",
        "print(\"Modèle RdmF\")\n",
        "\n",
        "\n",
        "\n",
        "def writeInCsvFile(Xtest_indice,ytestPrediction):\n",
        "    import csv\n",
        "    with open('prediction.csv', 'w', newline='') as prediction_file:\n",
        "        writer = csv.writer(prediction_file)\n",
        "        writer.writerow([\"ID\", \"Prediction\"])\n",
        "        for i in range(len(Xtest)):\n",
        "            writer.writerow([Xtest_indice[i], ytestPrediction[i]])\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "#Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.20)\n",
        "\n",
        "#Suppresion des colonnes inutiles\n",
        "\n",
        "Xtest = np.delete(Xtest,0,axis=1)\n",
        "Xtest = np.delete(Xtest,5,axis=1)\n",
        "Xtest = np.delete(Xtest,12,axis=1)\n",
        "Xtest = np.delete(Xtest,19,axis=1)\n",
        "#ytest = np.delete(ytest,0,axis=1)\n",
        "Xtrain = np.delete(Xtrain,0,axis=1)\n",
        "Xtrain = np.delete(Xtrain,5,axis=1)\n",
        "Xtrain = np.delete(Xtrain,12,axis=1)\n",
        "Xtrain = np.delete(Xtrain,19,axis=1)\n",
        "\n",
        "ytrain = np.delete(ytrain,0,axis=1)\n",
        "\n",
        "ytrain = np.ravel(ytrain)\n",
        "\n",
        "#dtree = DecisionTreeClassifier(criterion=\"entropy\")\n",
        "\n",
        "#ytest = np.ravel(ytest)\n",
        "\n",
        "print(\"Début de la phase d'entrainement\")\n",
        "\n",
        "\n"
      ],
      "id": "acknowledged-battery"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOLBm3XkR-Lo"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "xOLBm3XkR-Lo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aec29862",
        "outputId": "4731ff60-b748-4052-c7e3-e02ee95d162c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.19.5)\n",
            "Requirement already satisfied: parfit in /usr/local/lib/python3.7/dist-packages (0.220)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from parfit) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from parfit) (3.2.2)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from parfit) (0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from parfit) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->parfit) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->parfit) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->parfit) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->parfit) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->parfit) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->parfit) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->parfit) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->parfit) (1.4.1)\n",
            "Début de la phase de prédiction\n",
            "The time used to execute this is given below\n",
            "2223.7775197029114\n"
          ]
        }
      ],
      "source": [
        "from sklearn.kernel_approximation import RBFSampler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.kernel_approximation import Nystroem\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.metrics import roc_auc_score, make_scorer,accuracy_score\n",
        "!pip install xgboost\n",
        "!pip install parfit\n",
        "import xgboost as xgb\n",
        "import parfit.parfit as pf\n",
        "\n",
        "\n",
        "#X_train, X_val, y_train, y_val = train_test_split(Xtrain,ytrain, test_size=0.25)\n",
        "\n",
        "\n",
        "'''rbf_feature = Nystroem(n_components=600)\n",
        "Xtrain = rbf_feature.fit_transform(Xtrain)\n",
        "Xtest = rbf_feature.fit_transform(Xtest)'''\n",
        "\n",
        "#sgd = MLPClassifier(hidden_layer_sizes=200)\n",
        "\n",
        "#Utilisation du SGDClassifier, qui est plus rapide que MLP dans notre cas\n",
        "sgd = SGDClassifier(max_iter = 10000, alpha = 1, loss=\"log\",penalty=\"elasticnet\")\n",
        "\n",
        "'''params = {\n",
        "    \"criterion\" : ['gini', 'entropy'],\n",
        "    \"n_estimators\" : [50, 100, 150],\n",
        "    \"max_features\" : ['auto', 'sqrt', 'log2']\n",
        "}'''\n",
        "\n",
        "scorerr = make_scorer(roc_auc_score, multi_class=\"ovr\")\n",
        "\n",
        "'''params = ParameterGrid({\n",
        "    'learning_rate' : ['constant', 'optimal', 'adaptive', 'invscaling'] \n",
        " })'''\n",
        "\n",
        "params = ParameterGrid({\n",
        "         'learning_rate_init' : [0.001,0.01,0.1,0.5,1 ]\n",
        "     })\n",
        "\n",
        "\n",
        "'''best_model, best_score, all_models, all_scores = pf.bestFit(sgd, params, \n",
        "     X_train, y_train, X_val, y_val, \n",
        "     metric=accuracy_score, greater_is_better=True,scoreLabel='AUC')\n",
        "'''\n",
        "#print(best_model)\n",
        "\n",
        "\n",
        "\n",
        "sgd.fit(Xtrain, ytrain)\n",
        "\n",
        "print(\"Début de la phase de prédiction\")\n",
        "\n",
        "ytest_pred = sgd.predict(Xtest)\n",
        "\n",
        "#print(accuracy_score(ytest, ytest_pred_dtree))\n",
        "\n",
        "#error = 1 - sgd.score(Xtest, ytest)\n",
        "#print('Erreur: %f' % error)\n",
        "\n",
        "writeInCsvFile(Xtest_indice,ytest_pred)\n",
        "\n",
        "\n",
        "print(\"The time used to execute this is given below\")\n",
        "\n",
        "#print(hlv.best_estimator_)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(end - start)"
      ],
      "id": "aec29862"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhVqheb_0Yf4"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "vhVqheb_0Yf4"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "projetML.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}